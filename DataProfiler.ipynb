{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import IntegerType, FloatType, BooleanType, DateType\n",
    "import datetime\n",
    "\n",
    "def profile_and_cast_dataframe(df):\n",
    "    \"\"\"\n",
    "    Profiles a PySpark DataFrame with string columns and attempts to infer and cast columns to appropriate data types.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input PySpark DataFrame with all string columns.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with columns cast to inferred data types.\n",
    "    \"\"\"\n",
    "    def infer_data_type(value):\n",
    "        if value is None:\n",
    "            return None\n",
    "        try:\n",
    "            int(value)\n",
    "            return IntegerType()\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            float(value)\n",
    "            return FloatType()\n",
    "        except ValueError:\n",
    "            pass\n",
    "        if value.lower() in ['true', 'false']:\n",
    "            return BooleanType()\n",
    "        try:\n",
    "            datetime.datetime.strptime(value, '%Y-%m-%d')\n",
    "            return DateType()\n",
    "        except ValueError:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "    inferred_types = {}\n",
    "    for column in df.columns:\n",
    "        sample_values = df.select(column).distinct().limit(100).rdd.flatMap(lambda x: x).collect()\n",
    "        types = set(filter(None, (infer_data_type(value) for value in sample_values)))\n",
    "        \n",
    "        if len(types) == 1:\n",
    "            inferred_types[column] = types.pop()\n",
    "        else:\n",
    "            inferred_types[column] = None\n",
    "\n",
    "    for column, data_type in inferred_types.items():\n",
    "        if data_type == IntegerType():\n",
    "            df = df.withColumn(column, when(col(column).rlike('^-?\\\\d+$'), col(column).cast(IntegerType())))\n",
    "        elif data_type == FloatType():\n",
    "            df = df.withColumn(column, when(col(column).rlike('^-?\\\\d*\\\\.\\\\d+$'), col(column).cast(FloatType())))\n",
    "        elif data_type == BooleanType():\n",
    "            df = df.withColumn(column, when(col(column).rlike('^(true|false)$'), col(column).cast(BooleanType())))\n",
    "        elif data_type == DateType():\n",
    "            df = df.withColumn(column, when(col(column).rlike('^\\\\d{4}-\\\\d{2}-\\\\d{2}$'), col(column).cast(DateType())))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "\n",
    "def list_delta_tables_in_abfs_directory(spark, abfs_directory):\n",
    "    \"\"\"\n",
    "    Lists all Delta tables in a given ABFS directory.\n",
    "\n",
    "    Parameters:\n",
    "        spark (SparkSession): The active Spark session.\n",
    "        abfs_directory (str): The ABFS directory path (e.g., 'abfss://container@account.dfs.core.windows.net/path/').\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of Delta table paths found in the directory.\n",
    "    \"\"\"\n",
    "    # List all subdirectories in the ABFS directory\n",
    "    files = dbutils.fs.ls(abfs_directory)\n",
    "    delta_tables = []\n",
    "    for f in files:\n",
    "        # Check if _delta_log exists in the subdirectory (Delta table marker)\n",
    "        delta_log_path = os.path.join(f.path, \"_delta_log\")\n",
    "        if dbutils.fs.exists(delta_log_path):\n",
    "            delta_tables.append(f.path)\n",
    "    return delta_tables\n",
    "\n",
    "# Example usage:\n",
    "# abfs_directory = \"abfss://container@account.dfs.core.windows.net/source-directory/\"\n",
    "# delta_tables = list_delta_tables_in_abfs_directory(spark, abfs_directory)\n",
    "# print(delta_tables)\n",
    "def get_delta_table_key_columns(spark, table_path):\n",
    "    \"\"\"\n",
    "    Attempts to infer key columns for a Delta table by checking for columns with unique values.\n",
    "    Returns a list of candidate key columns (could be empty if none found).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(table_path)\n",
    "        columns = df.columns\n",
    "        row_count = df.count()\n",
    "        key_columns = []\n",
    "        for col_name in columns:\n",
    "            distinct_count = df.select(col_name).distinct().count()\n",
    "            if distinct_count == row_count:\n",
    "                key_columns.append(col_name)\n",
    "        return key_columns\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "# Example usage:\n",
    "for table_path in delta_tables:\n",
    "    key_columns = get_delta_table_key_columns(spark, table_path)\n",
    "    print(f\"Delta table: {table_path}\")\n",
    "    if len(key_columns) == 1:\n",
    "        print(f\"  Unique key column: {key_columns[0]}\")\n",
    "    elif key_columns:\n",
    "        print(f\"  Candidate key columns: {key_columns}\")\n",
    "    else:\n",
    "        print(\"  No unique key column found.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
